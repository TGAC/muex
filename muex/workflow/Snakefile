# Muex v0.0.1
# Kamil Hepak, Earlham Institute, 2023
# MIT License
################
### PREAMBLE ###
################

__author__ = "Kamil Hepak"

import pathlib

##############
### CONFIG ###
##############

configfile: "config.yaml"

# TODO READS_DIR
GENOME_REF_PATH = config["genome_ref_path"]
TRANS_REF_PATH = config["transcriptome_ref_path"]
ANNOT_PATH = config["annotation_path"]
TRANS_DICT_PATH = config["transcript_dict_path"]

MIN_SIZE = config.get("min_size","6")
MAX_SIZE = config.get("max_size","21")
UPFLANK = config.get("upflank","2")
DOWNFLANK = config.get("downflank","2")
LEEWAY = config.get("leeway","0")

THREADS = config.get("threads","8")

KMER_LEN = config.get("kmer_len","13")
MAX_GAP = config.get("max_gap","2000000")
MISMATCH_PEN = config.get("mismatch_pen","3")
GAP_OPEN_1 = config.get("gap_open_1","3")
GAP_OPEN_2 = config.get("gap_open_2","14")

num_files = len([x for x in list(pathlib.Path("reads").iterdir()) if x.is_file()])
IDS = [f"{x}" for x in range(num_files)]

# TODO rule to aggregate all X_microexons.tsv files into big csv, like capybara does
# TODO temp files / dirs

#############
### RULES ###
#############

rule all:
    input: 
        summary = expand("microexons/file{id}_microexons.tsv",id=IDS),
        fqc_html = expand("qc/file{id}_fastqc.html",id=IDS),
        fqc_zip = expand("qc/file{id}_fastqc.zip",id=IDS),

rule gen_microexons_full_summary:
    output: 
        summary = "microexons/{prefix}_microexons.tsv",
    input: 
        seqs = "info/{prefix}_microexons.fa",
        supporting_info = "info/{prefix}_supporting_info.tsv",
    params:
        upflank = UPFLANK,
        downflank = DOWNFLANK,
    script: "gen_summary.py"

rule extract_microexon_sequences:
    output:
        seqs = "info/{prefix}_microexons.fa",
    input:
        positions = "info/{prefix}_microexons_u2_d2.bed",
        reference = GENOME_REF_PATH,
    shell: "bedtools getfasta -s -fi {input.reference} -fo {output.seqs} -nameOnly -bed {input.positions}"

rule find_microexon_positions:
    output:
        positions = "info/{prefix}_microexons_u2_d2.bed", # TODO make u,d parametrised, pull from config
        supporting_info = "info/{prefix}_supporting_info.tsv",
    input:
        alignment = "alignments/{prefix}_genome.bam",
        reads_with_inserts = "inserts/{prefix}_reads_with_inserts.tsv",
    params:
        min_size = MIN_SIZE,
        max_size = MAX_SIZE,
        upflank = UPFLANK,
        downflank = DOWNFLANK,
        leeway = LEEWAY,
    script: "find_positions.py"

rule align_to_genome:
    output:
        alignment = "alignments/{prefix}_genome.bam",
    input:
        subset = "subsets/{prefix}_subset_full.fq", 
        reference = GENOME_REF_PATH,
    params:
        threads = THREADS,
        max_gap = MAX_GAP,
        mismatch_pen = MISMATCH_PEN,
        gap_open_1 = GAP_OPEN_1,
        gap_open_2 = GAP_OPEN_2,
        kmer_len = KMER_LEN,
    shell: "minimap2 -t {params.threads} --cs --secondary=yes -G {params.max_gap} -B{params.mismatch_pen} -O{params.gap_open_1},{params.gap_open_2} -k {params.kmer_len} -ax splice {input.reference} {input.subset} | samtools view -Sbo {output.alignment}"

rule subset_reads:
    output:
        subset = "subsets/{prefix}_subset_full.fq", # TODO gzip? (don't forget to edit input of align_to_genome)
    input: 
        readnames = "inserts/{prefix}_reads_to_extract.txt",
        alignments = "alignments/{prefix}_transcriptome.bam",
    shell: # TODO split this apart into multiple rules
        """set -x
        samtools view {input.alignments} | grep -Ff {input.readnames} > subsets/{wildcards.prefix}_subset_reads.sam
        samtools view -H {input.alignments} > subsets/{wildcards.prefix}_alignment_header.txt
        cat subsets/{wildcards.prefix}_alignment_header.txt subsets/{wildcards.prefix}_subset_reads.sam > subsets/{wildcards.prefix}_subset_full.sam
        samtools fastq subsets/{wildcards.prefix}_subset_full.sam > {output.subset}
        """

rule find_reads_with_inserts:
    output:
        readnames = "inserts/{prefix}_reads_to_extract.txt",
        reads_with_inserts = "inserts/{prefix}_reads_with_inserts.tsv",
    input:
        alignment = "alignments/{prefix}_transcriptome.bam",
        transcript_dict = TRANS_DICT_PATH,
    params:
        min_size = MIN_SIZE,
        max_size = MAX_SIZE,
    script: "find_inserts.py"

rule gen_transcript_dict:
    output:
        transcript_dict = TRANS_DICT_PATH,
    input:
        annotation = ANNOT_PATH,
    script: "gen_dict.py"

rule align_to_transcriptome:
    output:
        alignment = "alignments/{prefix}_transcriptome.bam",
    input:
        reads = "trimmed/{prefix}_trimmed.fq.gz",
        reference = TRANS_REF_PATH,
    params:
        threads = THREADS,
        kmer_len = KMER_LEN,
    shell: "minimap2 -t {params.threads} --cs --secondary=yes -k {params.kmer_len} -ax splice {input.reference} {input.reads} | samtools view -Sbo {output.alignment}"

rule trim_reads:
    output: 
        trimmed = "trimmed/{prefix}_trimmed.fq.gz",
    input: 
        reads = "reads/{prefix}.fq", # TODO make optional inputs of all extension types
    shell: "trim_galore --gzip {input.reads} -o trimmed/" # TODO does this definitely gzip output files

rule fastqc_reads:
    output: 
        html = "qc/{prefix}_fastqc.html",
        zipfile = "qc/{prefix}_fastqc.zip",
    input:
        reads = "reads/{prefix}.fq", # TODO extension-agnostic
    shell: "fastqc {input.reads} -o qc/"